{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tolgaerdogmus/wiki-text-preprocessing-visualization-nlp?scriptVersionId=193197007\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import re\nfrom collections import Counter\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport nltk\nimport string\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport spacy\nfrom textblob import Word, TextBlob\nfrom warnings import filterwarnings\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-19T13:32:34.491496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filterwarnings('ignore')\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', lambda x: '%.2f' % x)\npd.set_option('display.width', 200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nlp = spacy.load(\"en_core_web_sm\") \n\n# words = [\"run\", \"running\", \"ran\"] \n\n# stemmed_words = [nlp(word)[0].lemma_ for word in words] \n\n# print(stemmed_logs) \n# # prints: [\"run\", \"run\", \"run\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read data\ndf = pd.read_csv(\"/kaggle/input/wikimedia/wiki_data.csv\", index_col=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Take 2000 rows\ndf = df[:2000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#################################################\n# Görevler:\n#################################################\n\n# Görev 1: Metindeki ön işleme işlemlerini gerçekleştirecek bir fonksiyon yazınız.\n# •\tBüyük küçük harf dönüşümünü yapınız.\n# •\tNoktalama işaretlerini çıkarınız.\n# •\tNumerik ifadeleri çıkarınız.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Görev 2: Metin içinde öznitelik çıkarımı yaparken önemli olmayan kelimeleri çıkaracak fonksiyon yazınız.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('stopwords')\nnlp = spacy.load('en_core_web_sm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text_nltk(text, rare_words):\n    # Ensure text is a string\n    text = str(text)\n   \n    # Lowercase\n    text = text.lower()\n   \n    # Tokenize\n    tokens = word_tokenize(text)\n   \n    # Remove punctuation and numbers\n    tokens = [word for word in tokens if word.isalpha()]\n   \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n   \n    # Remove rare words\n    tokens = [word for word in tokens if word not in rare_words]\n   \n    # Join tokens back into string\n    cleaned_text = ' '.join(tokens)\n   \n    return cleaned_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_dataframe_text(df, text_column, min_word_frequency=100):\n    # Combine all text for word frequency calculation\n    all_text = ' '.join(df[text_column].astype(str))\n    \n    # Calculate word frequencies\n    word_freq = Counter(word_tokenize(all_text.lower()))\n    \n    # Identify rare words\n    rare_words = {word for word, count in word_freq.items() if count < min_word_frequency}\n    \n    # Clean each text entry\n    df['cleaned_text'] = df[text_column].apply(\n        lambda x: clean_text_nltk(x, rare_words)\n    )\n    \n    # Remove rows where cleaned text is empty\n    df = df[df['cleaned_text'] != ''].reset_index(drop=True)\n    \n    return df\n\n# Clean the entire dataframe\ndf = clean_dataframe_text(df, 'text', min_word_frequency=100)\n\nprint(f\"Shape of dataframe after cleaning: {df.shape}\")\nprint(f\"Sample of cleaned text:\\n{df['cleaned_text'].head()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean the entire dataframe\ndf = clean_dataframe_text(df, 'text', min_word_frequency=5000)\n\n# Or clean individual texts\n# df['text'] = df['text'].apply(lambda x: clean_text_nltk(x, min_word_frequency=1000))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Görev 3: Metinde az tekrarlayan kelimeleri bulunuz.\n# pd.Series(' '.join(df['text']).split()).value_counts()[-1000:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Görev 4: Metinde az tekrarlayan kelimeleri metin içerisinden çıkartınız. (İpucu: lambda fonksiyonunu kullanınız.)\n# already did above\n#sil = pd.Series(' '.join(df['text']).split()).value_counts()[-1000:]\n#df['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in sil))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Görev 5: Metinleri tokenize edip sonuçları gözlemleyiniz.\n\ndf[\"text\"].apply(lambda x: TextBlob(x).words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Görev 6: Lemmatization işlemini yapınız.\n# ran, runs, running -> run (normalleştirme)\n\n#df['text'] = df['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n\ndef spacy_stem(text):\n    doc = nlp(text)\n    return ' '.join([token.lemma_ for token in doc])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After your cleaning steps, apply stemming\ndf['text'] = df['text'].apply(spacy_stem)\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Görev 7: Metindeki terimlerin frekanslarını hesaplayınız. (İpucu: Barplot grafiği için gerekli)\n\ntf = df[\"text\"].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index() # kodu güncellemek gerekecek\n\ntf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Görev 8: Barplot grafiğini oluşturunuz.\n\n# Sütunların isimlendirilmesi\ntf.columns = [\"words\", \"tf\"]\n# 2000'den fazla geçen kelimelerin görselleştirilmesi\ntf[tf[\"tf\"] > 2000].plot.bar(x=\"words\", y=\"tf\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Kelimeleri WordCloud ile görselleştiriniz.\n\n# kelimeleri birleştirdik\ntext = \" \".join(i for i in df[\"text\"])\n\n# wordcloud görselleştirmenin özelliklerini belirliyoruz\nwordcloud = WordCloud(max_font_size=50,\nmax_words=100,\nbackground_color=\"black\").generate(text)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Görev 9: Tüm aşamaları tek bir fonksiyon olarak yazınız.\n# •\tMetin ön işleme işlemlerini gerçekleştiriniz.\n# •\tGörselleştirme işlemlerini fonksiyona argüman olarak ekleyiniz.\n# •\tFonksiyonu açıklayan 'docstring' yazınız.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport spacy\nfrom collections import Counter\n\nnltk.download('punkt', quiet=True)\nnltk.download('stopwords', quiet=True)\n\nnlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\nstop_words = set(stopwords.words('english'))\n\ndef wiki_preprocess(df, text_column, Barplot=False, Wordcloud=False):\n    \"\"\"\n    Preprocesses text data in a DataFrame and optionally creates visualizations.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame containing the text data.\n    text_column (str): The name of the column containing the text to be processed.\n    Barplot (bool): If True, creates a bar plot of word frequencies.\n    Wordcloud (bool): If True, creates a word cloud visualization.\n\n    Returns:\n    pandas.DataFrame: The DataFrame with an additional 'cleaned_text' column.\n    \"\"\"\n\n    def clean_and_tokenize(text):\n        text = str(text).lower()\n        return [word for word in word_tokenize(text) if word.isalpha() and word not in stop_words]\n\n    def process_text(tokens):\n        doc = nlp(' '.join(tokens))\n        return [token.lemma_ for token in doc]\n\n    # Apply cleaning and tokenization\n    df['tokens'] = df[text_column].apply(clean_and_tokenize)\n\n    # Apply stemming\n    df['cleaned_tokens'] = df['tokens'].apply(process_text)\n\n    # Count word frequencies\n    word_freq = Counter([word for tokens in df['cleaned_tokens'] for word in tokens])\n\n    # Remove rare words (appearing 5 times or less)\n    rare_words = {word for word, count in word_freq.items() if count <= 5000}\n    df['cleaned_text'] = df['cleaned_tokens'].apply(lambda tokens: ' '.join(word for word in tokens if word not in rare_words))\n\n    # Clean up intermediate columns\n    df = df.drop(['tokens', 'cleaned_tokens'], axis=1)\n\n    if Barplot or Wordcloud:\n        # Calculate word frequencies for visualization\n        vis_word_freq = Counter([word for text in df['cleaned_text'] for word in text.split()])\n\n    if Barplot:\n        # Create and display bar plot\n        top_words = dict(sorted(vis_word_freq.items(), key=lambda x: x[1], reverse=True)[:20])\n        plt.figure(figsize=(12, 6))\n        plt.bar(top_words.keys(), top_words.values())\n        plt.title(\"Top 20 Word Frequencies\")\n        plt.xlabel(\"Words\")\n        plt.ylabel(\"Frequency\")\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n        plt.show()\n\n    if Wordcloud:\n        # Create and display word cloud\n        wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate_from_frequencies(vis_word_freq)\n        plt.figure(figsize=(10, 5))\n        plt.imshow(wordcloud, interpolation=\"bilinear\")\n        plt.axis(\"off\")\n        plt.title(\"Word Cloud\")\n        plt.tight_layout()\n        plt.show()\n\n    return df\n\n# Usage\n# df = pd.read_csv(\"/kaggle/input/wikimedia/wiki_data.csv\", index_col=0)\n# processed_df = wiki_preprocess(df, 'text', Barplot=True, Wordcloud=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_df = wiki_preprocess(df, 'text', Barplot=True, Wordcloud=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}